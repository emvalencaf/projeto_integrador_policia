{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b24c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hdbscan import HDBSCAN\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import mlflow\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e0314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"../dataset\")\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "ARTIFACT_PATH = Path(\"../mlops\")\n",
    "OUTPUT_PATH = Path(\"../output\")\n",
    "VIOLENT_CRIMES = [\n",
    "    \"HOMICIDE\",\n",
    "    \"ASSAULT\",\n",
    "    \"BATTERY\",\n",
    "    \"ROBBERY\",\n",
    "    \"CRIMINAL SEXUAL ASSAULT\",\n",
    "    \"SEX OFFENSE\",\n",
    "    \"KIDNAPPING\",\n",
    "    \"INTIMIDATION\",\n",
    "    \"STALKING\",\n",
    "    \"OFFENSE INVOLVING CHILDREN\",\n",
    "    \"Sequestro\", \"Homicídio\", \"Estupro\",\n",
    "    \"Roubo\", \"Latrocínio\", \"Violência Doméstica\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15277cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///D:/senac-ads/projeto_integrador_policia/ml/mlops/751433899071555315', creation_time=1759455767505, experiment_id='751433899071555315', last_update_time=1759455767505, lifecycle_stage='active', name='Hotspot_Forecasting', tags={}>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(ARTIFACT_PATH)\n",
    "\n",
    "mlflow.set_experiment(\"Hotspot_Forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70763bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=200\n",
    "min_samples=60\n",
    "freq=\"D\"\n",
    "metric = \"haversine\"\n",
    "cluster_selection_method = \"eom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61d07c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH / \"dataset_ocorrencias_delegacia_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50d7a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_crimes_df = df#df[df[\"tipo_crime\"].isin(VIOLENT_CRIMES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "205082f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violent_crimes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ef331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_crimes_df[\"data_ocorrencia\"] = pd.to_datetime(violent_crimes_df[\"data_ocorrencia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa30d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = violent_crimes_df[['latitude', 'longitude']].dropna()\n",
    "coords_radians = np.radians(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "982f0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_key = \"recife\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da525833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo salvo: ..\\models\\recife\\hdbscan.pkl\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"HDBSCAN_Clustering\"):\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"min_cluster_size\": min_cluster_size,\n",
    "        \"min_samples\": min_samples,\n",
    "        \"metric\": \"haversine\",\n",
    "        \"cluster_selection_method\": cluster_selection_method,\n",
    "        \"prediction_data\": True\n",
    "    })\n",
    "    \n",
    "    labels = clusterer.fit_predict(coords_radians)\n",
    "    \n",
    "    \n",
    "    violent_crimes_df.loc[coords.index, \"hotspot_id\"] = labels\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # ignorando ruído\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    cluster_sizes = [list(labels).count(l) for l in set(labels) if l != -1]\n",
    "    mean_cluster_size = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "    max_cluster_size = np.max(cluster_sizes) if cluster_sizes else 0\n",
    "    min_cluster_size = np.min(cluster_sizes) if cluster_sizes else 0\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"n_noise_points\": n_noise,\n",
    "        \"mean_cluster_size\": mean_cluster_size,\n",
    "        \"max_cluster_size\": max_cluster_size,\n",
    "        \"min_cluster_size\": min_cluster_size\n",
    "    })\n",
    "    dir_path = MODEL_PATH / partition_key\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_file = dir_path / f\"hdbscan.pkl\"\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(clusterer, f)\n",
    "    print(f\"✓ Modelo salvo: {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f150f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_crimes_df.to_csv(OUTPUT_PATH / \"violent_crimes_chicago.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4192e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processando Hotspot: 1.0\n",
      "==================================================\n",
      "Total de registros: 1697\n",
      "Dias únicos na série temporal: 1697\n",
      "Treino: 1357 dias | Teste: 340 dias\n",
      "Treinando modelo AutoARIMA...\n",
      "✓ Modelo treinado\n",
      "MAE: 0.00 | RMSE: 0.00\n",
      "✓ Artifact salvo: forecast_hotspot_1.0.csv\n",
      "✓ Modelo salvo: ..\\models\\recife\\1.0_statsforecast.pkl\n",
      "✅ Hotspot 1.0 concluído com sucesso!\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processando Hotspot: 0.0\n",
      "==================================================\n",
      "Total de registros: 1278\n",
      "Dias únicos na série temporal: 1278\n",
      "Treino: 1022 dias | Teste: 256 dias\n",
      "Treinando modelo AutoARIMA...\n",
      "✓ Modelo treinado\n",
      "MAE: 0.00 | RMSE: 0.00\n",
      "✓ Artifact salvo: forecast_hotspot_0.0.csv\n",
      "✓ Modelo salvo: ..\\models\\recife\\0.0_statsforecast.pkl\n",
      "✅ Hotspot 0.0 concluído com sucesso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hotspot_id in violent_crimes_df[\"hotspot_id\"].dropna().unique():\n",
    "    if hotspot_id == -1:\n",
    "        continue\n",
    "    try:\n",
    "        with mlflow.start_run(run_name=f\"hotspot_{hotspot_id}\"):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processando Hotspot: {hotspot_id}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Filtra dados do hotspot\n",
    "            df_hotspot = violent_crimes_df[violent_crimes_df[\"hotspot_id\"] == hotspot_id]\n",
    "            print(f\"Total de registros: {len(df_hotspot)}\")\n",
    "            \n",
    "            # Agrega por data (contagem diária)\n",
    "            ts = df_hotspot.groupby(\"data_ocorrencia\").size().reset_index(name=\"y\")\n",
    "            ts[\"unique_id\"] = str(hotspot_id)\n",
    "            ts = ts.rename(columns={\"data_ocorrencia\": \"ds\"})\n",
    "            print(f\"Dias únicos na série temporal: {len(ts)}\")\n",
    "            \n",
    "            # Verificar se há dados suficientes\n",
    "            if len(ts) < 14:  # Mínimo para treino/teste\n",
    "                print(f\"⚠️ Hotspot {hotspot_id} tem poucos dados ({len(ts)} dias). Pulando...\")\n",
    "                mlflow.log_param(\"status\", \"skipped_insufficient_data\")\n",
    "                continue\n",
    "            \n",
    "            # Separar treino/teste\n",
    "            train_size = int(len(ts) * 0.8)\n",
    "            train, test = ts.iloc[:train_size], ts.iloc[train_size:]\n",
    "            print(f\"Treino: {len(train)} dias | Teste: {len(test)} dias\")\n",
    "            \n",
    "            # Treinar AutoARIMA\n",
    "            print(\"Treinando modelo AutoARIMA...\")\n",
    "            sf = StatsForecast(models=[AutoARIMA(season_length=7)], freq=\"D\", n_jobs=-1)\n",
    "            fcst_df = sf.forecast(df=train, h=len(test), fitted=True)\n",
    "            print(\"✓ Modelo treinado\")\n",
    "            \n",
    "            # Pegar previsão correta\n",
    "            y_pred = fcst_df.drop(columns=[\"unique_id\",\"ds\"]).iloc[:,0].values\n",
    "            y_true = test['y'].values\n",
    "            \n",
    "            # Métricas\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            \n",
    "            print(f\"MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "            \n",
    "            # Salvar métricas no MLflow\n",
    "            mlflow.log_metrics({\n",
    "                \"MAE\": mae,\n",
    "                \"RMSE\": rmse,\n",
    "                \"train_size\": len(train),\n",
    "                \"test_size\": len(test)\n",
    "            })\n",
    "            \n",
    "            # Logar parâmetros básicos\n",
    "            mlflow.log_params({\n",
    "                \"model\": \"AutoARIMA\",\n",
    "                \"season_length\": 7,\n",
    "                \"freq\": \"D\",\n",
    "                \"hotspot_id\": str(hotspot_id)\n",
    "            })\n",
    "            \n",
    "            # Salvar previsões como artifact\n",
    "            forecast_file = f\"forecast_hotspot_{hotspot_id}.csv\"\n",
    "            fcst_df.to_csv(forecast_file, index=False)\n",
    "            mlflow.log_artifact(forecast_file)\n",
    "            print(f\"✓ Artifact salvo: {forecast_file}\")\n",
    "            \n",
    "            # Salvar o StatsForecast completo\n",
    "            dir_path = MODEL_PATH / partition_key\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            model_file = dir_path / f\"{hotspot_id}_statsforecast.pkl\"\n",
    "            \n",
    "            with open(model_file, 'wb') as f:\n",
    "                pickle.dump(sf, f)\n",
    "            \n",
    "            print(f\"✓ Modelo salvo: {model_file}\")\n",
    "            \n",
    "            print(f\"✅ Hotspot {hotspot_id} concluído com sucesso!\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERRO ao processar hotspot {hotspot_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Tentar logar o erro no MLflow\n",
    "        try:\n",
    "            mlflow.log_param(\"status\", \"failed\")\n",
    "            mlflow.log_param(\"error\", str(e)[:250])  # MLflow tem limite de chars\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        continue  # Continua para o próximo hotspot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
